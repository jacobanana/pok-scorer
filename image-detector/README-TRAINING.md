# POK Detector Training Pipeline

This document explains how to train the POK detector parameters using the CI/CD pipeline.

## Overview

The training pipeline uses **Bayesian Optimization** to automatically tune detection parameters for optimal accuracy. Training runs in GitHub Actions CI, eliminating the need for manual browser-based calibration.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Training Pipeline                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Annotate Dataset (Browser)                          â”‚
â”‚     â†’ annotation-editor.html                            â”‚
â”‚     â†’ Export dataset.json + images                      â”‚
â”‚                                                          â”‚
â”‚  2. Commit to Repository                                â”‚
â”‚     â†’ image-detector/datasets/                          â”‚
â”‚       â”œâ”€â”€ pok-training.json                            â”‚
â”‚       â””â”€â”€ images/                                       â”‚
â”‚           â”œâ”€â”€ game1.jpg                                â”‚
â”‚           â””â”€â”€ game2.jpg                                â”‚
â”‚                                                          â”‚
â”‚  3. CI Training (GitHub Actions)                        â”‚
â”‚     â†’ Bayesian Optimization (500 iterations)           â”‚
â”‚     â†’ Validation on test set                           â”‚
â”‚     â†’ Commit optimized parameters                      â”‚
â”‚                                                          â”‚
â”‚  4. Deployment                                          â”‚
â”‚     â†’ image-detector/models/detector-params.json       â”‚
â”‚     â†’ Auto-deployed to GitHub Pages                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Step 1: Create Training Dataset

1. Open [annotation-editor.html](annotation-editor.html) in your browser
2. Load images of POK games
3. Annotate poks (click to add, click again to cycle red â†’ blue â†’ delete)
4. Use Shift+Drag to move, Ctrl+Drag to resize
5. Click **"Download Dataset"** to export JSON
6. Click **"Save to Storage"** to persist locally

### Step 2: Prepare Dataset for CI

Create the following structure in your repository:

```
image-detector/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ pok-training.json       â† Your exported dataset
â”‚   â””â”€â”€ images/                  â† Your training images
â”‚       â”œâ”€â”€ game1.jpg
â”‚       â”œâ”€â”€ game2.jpg
â”‚       â””â”€â”€ ...
â””â”€â”€ models/
    â””â”€â”€ detector-params.json     â† Generated by CI
```

**Minimum Requirements:**
- 20-30 annotated images for basic accuracy
- 50+ images recommended for production use
- Diverse scenarios: different lighting, angles, table types

### Step 3: Trigger CI Training

#### Option A: Automatic (on commit)

```bash
git add image-detector/datasets/
git commit -m "Add training dataset"
git push
```

The workflow automatically triggers when you push changes to `image-detector/datasets/`.

#### Option B: Manual Trigger

1. Go to **Actions** tab in GitHub
2. Select **"Train POK Detector"** workflow
3. Click **"Run workflow"**
4. Set parameters:
   - Dataset path: `image-detector/datasets/pok-training.json`
   - Iterations: `500` (default)
5. Click **"Run workflow"**

### Step 4: Monitor Training

Watch the workflow progress in the Actions tab. Training takes ~10-15 minutes for 500 iterations.

Example output:
```
ğŸ“Š Dataset loaded: 30 images
   Training: 21 images
   Validation: 9 images

ğŸš€ Starting Bayesian Optimization...

Iteration 1/500: Score = 45.2
Iteration 50/500: Score = 67.8 (new best!)
Iteration 100/500: Score = 72.4 (new best!)
...
Iteration 500/500: Score = 81.2

ğŸ“Š FINAL RESULTS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Score:   81.2                                      â”‚
â”‚ Validation Score: 78.9                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ F1 Score:         85.3%                                     â”‚
â”‚ Color Accuracy:   92.1%                                     â”‚
â”‚ Precision:        88.7%                                     â”‚
â”‚ Recall:           82.3%                                     â”‚
â”‚ Avg Pos Error:    12.4px                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… VALIDATION PASSED
âœ… Parameters saved to: image-detector/models/detector-params.json
```

### Step 5: Use Optimized Parameters

The trained parameters are automatically:
- Committed to the repository
- Tagged with timestamp (e.g., `detector-params-20250601-143522`)
- Deployed to GitHub Pages
- Available at: `https://<username>.github.io/pok-scorer/image-detector/`

To use locally:
1. Open [index.html](index.html) (detector)
2. Parameters are auto-loaded from `models/detector-params.json`
3. Or manually import via **"Import Parameters"** button

## Training Scripts Reference

### calibrate.py

Main training script using Bayesian Optimization.

**Usage:**
```bash
python image-detector/scripts/calibrate.py \
  --dataset image-detector/datasets/pok-training.json \
  --images image-detector/datasets/images \
  --output image-detector/models/detector-params.json \
  --iterations 500 \
  --match-threshold 50
```

**Arguments:**
- `--dataset`: Path to dataset JSON
- `--images`: Directory containing training images
- `--output`: Output path for optimized parameters
- `--iterations`: Number of Bayesian optimization calls (default: 500)
- `--starting-params`: Optional starting parameters JSON
- `--match-threshold`: Distance threshold in pixels for matching detections to annotations (default: 50)

**Algorithm:**
- Uses Gaussian Process-based Bayesian Optimization (scikit-optimize)
- Smarter than random search: learns from previous evaluations
- Explores parameter space efficiently
- Converges 3-5x faster than browser-based random + hill climbing

### validate.py

Validates trained parameters against a test dataset.

**Usage:**
```bash
python image-detector/scripts/validate.py \
  --params image-detector/models/detector-params.json \
  --dataset image-detector/datasets/pok-test.json \
  --images image-detector/datasets/images \
  --min-f1 0.7 \
  --min-color-accuracy 0.7
```

**Arguments:**
- `--params`: Path to parameters JSON to validate
- `--dataset`: Path to test dataset JSON
- `--images`: Directory containing test images
- `--min-f1`: Minimum required F1 score (default: 0.7)
- `--min-color-accuracy`: Minimum required color accuracy (default: 0.7)
- `--match-threshold`: Distance threshold in pixels (default: 50)

**Exit Codes:**
- `0`: Validation passed
- `1`: Validation failed (metrics below threshold)

## Parameter Space

The optimizer searches these ranges:

### Circle Detection (HoughCircles)
- `dp`: 1.0 - 2.5 (resolution scale)
- `minDist`: 10 - 80 px (min distance between circles)
- `param1`: 30 - 200 (Canny edge threshold)
- `param2`: 10 - 60 (circle detection threshold)
- `minRadius`: 5 - 50 px
- `maxRadius`: 20 - 100 px

### Color Classification (HSV)
**Red (wraps around 0/180):**
- `redH1Low`: 0 - 10
- `redH1High`: 5 - 20
- `redH2Low`: 150 - 175
- `redH2High`: 170 - 180
- `redSMin`: 50 - 180 (saturation)
- `redVMin`: 50 - 180 (value/brightness)

**Blue:**
- `blueH1Low`: 90 - 115
- `blueH1High`: 115 - 140
- `blueSMin`: 50 - 180
- `blueVMin`: 50 - 180

## Local Development

### Setup

```bash
cd image-detector/scripts

# Install dependencies with uv
uv pip install -r pyproject.toml

# Or use pip
pip install -r pyproject.toml
```

### Run Training Locally

```bash
# Basic training
python calibrate.py \
  --dataset ../datasets/pok-training.json \
  --images ../datasets/images \
  --output ../models/detector-params.json

# Fast iteration (fewer calls)
python calibrate.py \
  --dataset ../datasets/pok-training.json \
  --images ../datasets/images \
  --output ../models/test-params.json \
  --iterations 50

# Continue from existing parameters
python calibrate.py \
  --dataset ../datasets/pok-training.json \
  --images ../datasets/images \
  --output ../models/detector-params-v2.json \
  --starting-params ../models/detector-params.json \
  --iterations 200
```

### Run Validation Locally

```bash
python validate.py \
  --params ../models/detector-params.json \
  --dataset ../datasets/pok-test.json \
  --images ../datasets/images \
  --min-f1 0.75 \
  --min-color-accuracy 0.80
```

## Metrics Explained

### F1 Score
Harmonic mean of precision and recall. Balances false positives and false negatives.
- **Good**: â‰¥ 80%
- **Acceptable**: 70-80%
- **Poor**: < 70%

### Precision
Percentage of detections that match real poks (low false positives).
```
Precision = True Positives / (True Positives + False Positives)
```

### Recall
Percentage of real poks that were detected (low false negatives).
```
Recall = True Positives / (True Positives + False Negatives)
```

### Color Accuracy
Percentage of correctly classified pok colors (red vs blue).
- **Good**: â‰¥ 85%
- **Acceptable**: 75-85%
- **Poor**: < 75%

### Position Error
Average pixel distance between detected and annotated pok centers.
- **Good**: < 15px
- **Acceptable**: 15-30px
- **Poor**: > 30px

## Troubleshooting

### Issue: Low F1 Score (< 70%)

**Causes:**
- Insufficient training data
- Images too different from test conditions
- Parameter space may need adjustment

**Solutions:**
1. Add more diverse training images (different lighting, angles)
2. Increase iterations to 1000+
3. Check image quality (blur, resolution)
4. Verify annotations are accurate

### Issue: Low Color Accuracy (< 80%)

**Causes:**
- HSV ranges too narrow for lighting variations
- Poks have faded/damaged colors
- Metallic center interfering with classification

**Solutions:**
1. Add images with varied lighting conditions
2. Check that annotations match actual pok colors
3. Consider manual HSV range adjustment
4. Increase saturation/value threshold flexibility

### Issue: High False Positives

**Causes:**
- `param2` (circle threshold) too low
- Detecting buttons, coins, other circular objects

**Solutions:**
1. Increase minimum `param2` in parameter space
2. Add negative examples (images without poks)
3. Refine radius constraints (`minRadius`, `maxRadius`)

### Issue: High False Negatives

**Causes:**
- `param2` too high (missing valid poks)
- Radius constraints too strict
- Images have motion blur or poor focus

**Solutions:**
1. Decrease minimum `param2`
2. Widen radius range
3. Use higher quality images

### Issue: CI Workflow Fails

**Check:**
1. Dataset JSON exists at specified path
2. Images directory exists and contains JPG/PNG files
3. All filenames in JSON match actual image files
4. JSON is valid (use [jsonlint.com](https://jsonlint.com))

## Advanced Usage

### Cross-Validation

For more robust parameter estimation, split your dataset into multiple folds:

```python
# Example: 5-fold cross-validation
from pathlib import Path
import json

dataset_path = Path('image-detector/datasets/pok-training.json')
with open(dataset_path) as f:
    dataset = json.load(f)

images = dataset['images']
fold_size = len(images) // 5

for fold in range(5):
    # Create train/test split for this fold
    test_start = fold * fold_size
    test_end = test_start + fold_size

    test_images = images[test_start:test_end]
    train_images = images[:test_start] + images[test_end:]

    # Save fold datasets
    train_fold = {'version': '1.0', 'images': train_images}
    test_fold = {'version': '1.0', 'images': test_images}

    with open(f'datasets/fold{fold}-train.json', 'w') as f:
        json.dump(train_fold, f)
    with open(f'datasets/fold{fold}-test.json', 'w') as f:
        json.dump(test_fold, f)

# Train on each fold and average results
```

### Ensemble Parameters

Train multiple models and use voting:

```bash
# Train 3 different models
for i in {1..3}; do
  python calibrate.py \
    --dataset datasets/pok-training.json \
    --images datasets/images \
    --output models/detector-params-model$i.json \
    --iterations 500
done

# Average the parameters (custom script needed)
python scripts/ensemble_params.py \
  models/detector-params-model*.json \
  --output models/detector-params-ensemble.json
```

## Performance Benchmarks

Based on 30 training images, 500 iterations:

| Metric | Value |
|--------|-------|
| Training time | 10-15 minutes (GitHub Actions) |
| CI cost | ~$0.08 per run (private repos) |
| Expected F1 score | 75-85% |
| Expected color accuracy | 85-95% |
| Convergence rate | 3-5x faster than random search |

## Next Steps

1. **Collect more data**: Aim for 50+ images for production use
2. **Test diverse conditions**: Indoor/outdoor, different tables, lighting
3. **Iterate**: Retrain when accuracy drops on new images
4. **Monitor**: Track F1 scores over time, retrain when < 75%

## Comparison: CI Training vs Browser Training

| Feature | CI Training (Python) | Browser Training (JS) |
|---------|---------------------|----------------------|
| Algorithm | Bayesian Optimization | Random + Hill Climbing |
| Speed | 10-15 min (500 iter) | 20-30 min (150 iter) |
| Convergence | 3-5x faster | Baseline |
| Reproducibility | âœ… Deterministic | âŒ Random seed varies |
| Version control | âœ… Auto-committed | âš ï¸ Manual export |
| Collaboration | âœ… Team training | âŒ Local only |
| CI/CD integration | âœ… Automated | âŒ Manual |

## Support

For issues or questions:
- Open an issue on GitHub
- Check workflow logs in Actions tab
- Review validation output for failure reasons
